{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde90b11",
   "metadata": {},
   "source": [
    "# Order Amount Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ac9c1",
   "metadata": {},
   "source": [
    "# Milestone 1 - Data Sanity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643e65e",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8009ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt #Module used for working with date and time\n",
    "import numpy as np #Library used for calculation of mathematical numericals\n",
    "import pandas as pd #Library used for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d7bfa",
   "metadata": {},
   "source": [
    "# 1) Use the PRS dataset to create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d345983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Input_Dataset.csv') #Read a csv file and store its content in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44957aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ORDER_ID</th>\n",
       "      <th>SALES_ORG</th>\n",
       "      <th>DISTRIBUTION_CHANNEL</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>RELEASED_CREDIT_VALUE</th>\n",
       "      <th>PURCHASE_ORDER_TYPE</th>\n",
       "      <th>COMPANY_CODE</th>\n",
       "      <th>ORDER_CREATION_DATE</th>\n",
       "      <th>ORDER_CREATION_TIME</th>\n",
       "      <th>CREDIT_CONTROL_AREA</th>\n",
       "      <th>SOLD_TO_PARTY</th>\n",
       "      <th>ORDER_AMOUNT</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>ORDER_CURRENCY</th>\n",
       "      <th>CREDIT_STATUS</th>\n",
       "      <th>CUSTOMER_NUMBER</th>\n",
       "      <th>amount_in_usd</th>\n",
       "      <th>unique_cust_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946851639</td>\n",
       "      <td>3537</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>43012</td>\n",
       "      <td>5</td>\n",
       "      <td>756141537</td>\n",
       "      <td>954.61</td>\n",
       "      <td>2022-01-13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6657</td>\n",
       "      <td>1041.014177</td>\n",
       "      <td>6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309984</th>\n",
       "      <td>931698777</td>\n",
       "      <td>3222</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>160343</td>\n",
       "      <td>0</td>\n",
       "      <td>879128357</td>\n",
       "      <td>137.36</td>\n",
       "      <td>2022-01-12</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>137.360000</td>\n",
       "      <td>5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309983</th>\n",
       "      <td>766285206</td>\n",
       "      <td>2840</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>160245</td>\n",
       "      <td>7</td>\n",
       "      <td>975590823</td>\n",
       "      <td>993.10</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>993.100000</td>\n",
       "      <td>5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309982</th>\n",
       "      <td>894767639</td>\n",
       "      <td>2769</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>160205</td>\n",
       "      <td>4</td>\n",
       "      <td>877241643</td>\n",
       "      <td>460.61</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>460.610000</td>\n",
       "      <td>5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309981</th>\n",
       "      <td>791798584</td>\n",
       "      <td>3319</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>160017</td>\n",
       "      <td>2</td>\n",
       "      <td>960261867</td>\n",
       "      <td>1024.53</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>1024.530000</td>\n",
       "      <td>5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001972</th>\n",
       "      <td>770014438</td>\n",
       "      <td>4051</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>211800</td>\n",
       "      <td>6</td>\n",
       "      <td>968730991</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001971</th>\n",
       "      <td>809082888</td>\n",
       "      <td>2355</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>211759</td>\n",
       "      <td>1</td>\n",
       "      <td>759923943</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001970</th>\n",
       "      <td>812449310</td>\n",
       "      <td>4328</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>211758</td>\n",
       "      <td>6</td>\n",
       "      <td>983363037</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001996</th>\n",
       "      <td>814037743</td>\n",
       "      <td>3482</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>211824</td>\n",
       "      <td>7</td>\n",
       "      <td>897719234</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101924</th>\n",
       "      <td>921701000</td>\n",
       "      <td>2968</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>26</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>113357</td>\n",
       "      <td>1</td>\n",
       "      <td>754385277</td>\n",
       "      <td>4415.19</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2811</td>\n",
       "      <td>257.400000</td>\n",
       "      <td>2813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101925 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CUSTOMER_ORDER_ID  SALES_ORG  DISTRIBUTION_CHANNEL  DIVISION  \\\n",
       "0                946851639       3537                   232         1   \n",
       "309984           931698777       3222                   116         0   \n",
       "309983           766285206       2840                    40         0   \n",
       "309982           894767639       2769                    75         0   \n",
       "309981           791798584       3319                   102         0   \n",
       "...                    ...        ...                   ...       ...   \n",
       "1001972          770014438       4051                   152         0   \n",
       "1001971          809082888       2355                   203         0   \n",
       "1001970          812449310       4328                   103         0   \n",
       "1001996          814037743       3482                     5         0   \n",
       "1101924          921701000       2968                   156         1   \n",
       "\n",
       "         RELEASED_CREDIT_VALUE  PURCHASE_ORDER_TYPE  COMPANY_CODE  \\\n",
       "0                            1                    0            10   \n",
       "309984                       1                  343            22   \n",
       "309983                       1                  343            22   \n",
       "309982                       1                  343            22   \n",
       "309981                       1                  343            22   \n",
       "...                        ...                  ...           ...   \n",
       "1001972                      1                  343            22   \n",
       "1001971                      1                  343            22   \n",
       "1001970                      1                  343            22   \n",
       "1001996                      1                  343            22   \n",
       "1101924                      1                  306            26   \n",
       "\n",
       "        ORDER_CREATION_DATE  ORDER_CREATION_TIME  CREDIT_CONTROL_AREA  \\\n",
       "0                2022-01-01                43012                    5   \n",
       "309984           2022-01-01               160343                    0   \n",
       "309983           2022-01-01               160245                    7   \n",
       "309982           2022-01-01               160205                    4   \n",
       "309981           2022-01-01               160017                    2   \n",
       "...                     ...                  ...                  ...   \n",
       "1001972          2022-06-01               211800                    6   \n",
       "1001971          2022-06-01               211759                    1   \n",
       "1001970          2022-06-01               211758                    6   \n",
       "1001996          2022-06-01               211824                    7   \n",
       "1101924          2022-06-01               113357                    1   \n",
       "\n",
       "         SOLD_TO_PARTY  ORDER_AMOUNT REQUESTED_DELIVERY_DATE  ORDER_CURRENCY  \\\n",
       "0            756141537        954.61              2022-01-13               6   \n",
       "309984       879128357        137.36              2022-01-12              19   \n",
       "309983       975590823        993.10              2022-01-11              19   \n",
       "309982       877241643        460.61              2022-01-11              19   \n",
       "309981       960261867       1024.53              2022-01-10              19   \n",
       "...                ...           ...                     ...             ...   \n",
       "1001972      968730991          0.00              2022-06-10              19   \n",
       "1001971      759923943          0.00              2022-06-10              19   \n",
       "1001970      983363037          0.00              2022-06-10              19   \n",
       "1001996      897719234          0.00              2022-06-10              19   \n",
       "1101924      754385277       4415.19              2022-06-01              18   \n",
       "\n",
       "         CREDIT_STATUS  CUSTOMER_NUMBER  amount_in_usd  unique_cust_id  \n",
       "0                    3             6657    1041.014177            6664  \n",
       "309984               3             5941     137.360000            5948  \n",
       "309983               3             5941     993.100000            5948  \n",
       "309982               3             5941     460.610000            5948  \n",
       "309981               3             5941    1024.530000            5948  \n",
       "...                ...              ...            ...             ...  \n",
       "1001972              3             5888       0.000000            5895  \n",
       "1001971              3             5888       0.000000            5895  \n",
       "1001970              3             5888       0.000000            5895  \n",
       "1001996              3             5888       0.000000            5895  \n",
       "1101924              3             2811     257.400000            2813  \n",
       "\n",
       "[1101925 rows x 18 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #Printing the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ea6a2",
   "metadata": {},
   "source": [
    "# 2.Check the description of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "258a2727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ORDER_ID</th>\n",
       "      <th>SALES_ORG</th>\n",
       "      <th>DISTRIBUTION_CHANNEL</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>RELEASED_CREDIT_VALUE</th>\n",
       "      <th>PURCHASE_ORDER_TYPE</th>\n",
       "      <th>COMPANY_CODE</th>\n",
       "      <th>ORDER_CREATION_TIME</th>\n",
       "      <th>CREDIT_CONTROL_AREA</th>\n",
       "      <th>SOLD_TO_PARTY</th>\n",
       "      <th>ORDER_AMOUNT</th>\n",
       "      <th>ORDER_CURRENCY</th>\n",
       "      <th>CREDIT_STATUS</th>\n",
       "      <th>CUSTOMER_NUMBER</th>\n",
       "      <th>amount_in_usd</th>\n",
       "      <th>unique_cust_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "      <td>1.100548e+06</td>\n",
       "      <td>1.101925e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.763187e+08</td>\n",
       "      <td>3.304891e+03</td>\n",
       "      <td>1.205879e+02</td>\n",
       "      <td>4.681562e-01</td>\n",
       "      <td>1.626244e+03</td>\n",
       "      <td>2.574177e+02</td>\n",
       "      <td>1.791817e+01</td>\n",
       "      <td>1.340316e+05</td>\n",
       "      <td>3.503778e+00</td>\n",
       "      <td>8.763265e+08</td>\n",
       "      <td>5.550257e+03</td>\n",
       "      <td>1.332469e+01</td>\n",
       "      <td>2.670039e+00</td>\n",
       "      <td>5.605647e+03</td>\n",
       "      <td>3.436395e+02</td>\n",
       "      <td>5.612079e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.088594e+07</td>\n",
       "      <td>6.958920e+02</td>\n",
       "      <td>7.002109e+01</td>\n",
       "      <td>4.989852e-01</td>\n",
       "      <td>6.955082e+03</td>\n",
       "      <td>1.156873e+02</td>\n",
       "      <td>6.459448e+00</td>\n",
       "      <td>6.503618e+04</td>\n",
       "      <td>2.292382e+00</td>\n",
       "      <td>7.090306e+07</td>\n",
       "      <td>2.448873e+05</td>\n",
       "      <td>7.005648e+00</td>\n",
       "      <td>7.032286e-01</td>\n",
       "      <td>1.159312e+03</td>\n",
       "      <td>3.943413e+02</td>\n",
       "      <td>1.160876e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.534520e+08</td>\n",
       "      <td>2.100000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.534518e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.149522e+08</td>\n",
       "      <td>2.702000e+03</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>8.170200e+04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>8.148366e+08</td>\n",
       "      <td>1.260000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.826000e+03</td>\n",
       "      <td>1.257361e+01</td>\n",
       "      <td>5.833000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.763963e+08</td>\n",
       "      <td>3.305000e+03</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.430000e+02</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>1.509540e+05</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>8.763542e+08</td>\n",
       "      <td>2.761200e+02</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.880000e+03</td>\n",
       "      <td>2.574000e+02</td>\n",
       "      <td>5.887000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.376832e+08</td>\n",
       "      <td>3.908000e+03</td>\n",
       "      <td>1.810000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.440000e+02</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.853580e+05</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>9.377293e+08</td>\n",
       "      <td>7.921000e+02</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.942000e+03</td>\n",
       "      <td>4.905311e+02</td>\n",
       "      <td>5.949000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.990063e+08</td>\n",
       "      <td>4.510000e+03</td>\n",
       "      <td>2.420000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.790300e+04</td>\n",
       "      <td>3.500000e+02</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>2.359590e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>9.990075e+08</td>\n",
       "      <td>1.286378e+08</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>6.747000e+03</td>\n",
       "      <td>1.790160e+03</td>\n",
       "      <td>6.754000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CUSTOMER_ORDER_ID     SALES_ORG  DISTRIBUTION_CHANNEL      DIVISION  \\\n",
       "count       1.101925e+06  1.101925e+06          1.101925e+06  1.101925e+06   \n",
       "mean        8.763187e+08  3.304891e+03          1.205879e+02  4.681562e-01   \n",
       "std         7.088594e+07  6.958920e+02          7.002109e+01  4.989852e-01   \n",
       "min         7.534520e+08  2.100000e+03          0.000000e+00  0.000000e+00   \n",
       "25%         8.149522e+08  2.702000e+03          6.000000e+01  0.000000e+00   \n",
       "50%         8.763963e+08  3.305000e+03          1.200000e+02  0.000000e+00   \n",
       "75%         9.376832e+08  3.908000e+03          1.810000e+02  1.000000e+00   \n",
       "max         9.990063e+08  4.510000e+03          2.420000e+02  1.000000e+00   \n",
       "\n",
       "       RELEASED_CREDIT_VALUE  PURCHASE_ORDER_TYPE  COMPANY_CODE  \\\n",
       "count           1.101925e+06         1.101925e+06  1.101925e+06   \n",
       "mean            1.626244e+03         2.574177e+02  1.791817e+01   \n",
       "std             6.955082e+03         1.156873e+02  6.459448e+00   \n",
       "min             0.000000e+00         0.000000e+00  0.000000e+00   \n",
       "25%             1.000000e+00         1.350000e+02  1.200000e+01   \n",
       "50%             1.000000e+00         3.430000e+02  2.200000e+01   \n",
       "75%             1.000000e+00         3.440000e+02  2.300000e+01   \n",
       "max             4.790300e+04         3.500000e+02  2.700000e+01   \n",
       "\n",
       "       ORDER_CREATION_TIME  CREDIT_CONTROL_AREA  SOLD_TO_PARTY  ORDER_AMOUNT  \\\n",
       "count         1.101925e+06         1.101925e+06   1.101925e+06  1.101925e+06   \n",
       "mean          1.340316e+05         3.503778e+00   8.763265e+08  5.550257e+03   \n",
       "std           6.503618e+04         2.292382e+00   7.090306e+07  2.448873e+05   \n",
       "min           0.000000e+00         0.000000e+00   7.534518e+08  0.000000e+00   \n",
       "25%           8.170200e+04         2.000000e+00   8.148366e+08  1.260000e+01   \n",
       "50%           1.509540e+05         4.000000e+00   8.763542e+08  2.761200e+02   \n",
       "75%           1.853580e+05         6.000000e+00   9.377293e+08  7.921000e+02   \n",
       "max           2.359590e+05         7.000000e+00   9.990075e+08  1.286378e+08   \n",
       "\n",
       "       ORDER_CURRENCY  CREDIT_STATUS  CUSTOMER_NUMBER  amount_in_usd  \\\n",
       "count    1.101925e+06   1.101925e+06     1.101925e+06   1.100548e+06   \n",
       "mean     1.332469e+01   2.670039e+00     5.605647e+03   3.436395e+02   \n",
       "std      7.005648e+00   7.032286e-01     1.159312e+03   3.943413e+02   \n",
       "min      0.000000e+00   0.000000e+00     0.000000e+00   0.000000e+00   \n",
       "25%      6.000000e+00   3.000000e+00     5.826000e+03   1.257361e+01   \n",
       "50%      1.900000e+01   3.000000e+00     5.880000e+03   2.574000e+02   \n",
       "75%      1.900000e+01   3.000000e+00     5.942000e+03   4.905311e+02   \n",
       "max      1.900000e+01   3.000000e+00     6.747000e+03   1.790160e+03   \n",
       "\n",
       "       unique_cust_id  \n",
       "count    1.101925e+06  \n",
       "mean     5.612079e+03  \n",
       "std      1.160876e+03  \n",
       "min      0.000000e+00  \n",
       "25%      5.833000e+03  \n",
       "50%      5.887000e+03  \n",
       "75%      5.949000e+03  \n",
       "max      6.754000e+03  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() #Describing the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae46e69",
   "metadata": {},
   "source": [
    "# 3. Check the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "995cb291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101925, 18)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #Gives the shape of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dba337",
   "metadata": {},
   "source": [
    "# 4. Check the data frame informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "858d3747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1101925 entries, 0 to 1101924\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count    Dtype         \n",
      "---  ------                   --------------    -----         \n",
      " 0   CUSTOMER_ORDER_ID        1101925 non-null  int64         \n",
      " 1   SALES_ORG                1101925 non-null  int64         \n",
      " 2   DISTRIBUTION_CHANNEL     1101925 non-null  int32         \n",
      " 3   DIVISION                 1101925 non-null  int32         \n",
      " 4   RELEASED_CREDIT_VALUE    1101925 non-null  int32         \n",
      " 5   PURCHASE_ORDER_TYPE      1101925 non-null  int32         \n",
      " 6   COMPANY_CODE             1101925 non-null  int32         \n",
      " 7   ORDER_CREATION_DATE      1101925 non-null  datetime64[ns]\n",
      " 8   ORDER_CREATION_TIME      1101925 non-null  int64         \n",
      " 9   CREDIT_CONTROL_AREA      1101925 non-null  int32         \n",
      " 10  SOLD_TO_PARTY            1101925 non-null  int64         \n",
      " 11  ORDER_AMOUNT             1101925 non-null  float64       \n",
      " 12  REQUESTED_DELIVERY_DATE  1101925 non-null  datetime64[ns]\n",
      " 13  ORDER_CURRENCY           1101925 non-null  int32         \n",
      " 14  CREDIT_STATUS            1101925 non-null  int32         \n",
      " 15  CUSTOMER_NUMBER          1101925 non-null  int32         \n",
      " 16  amount_in_usd            1100548 non-null  float64       \n",
      " 17  unique_cust_id           1101925 non-null  int32         \n",
      "dtypes: datetime64[ns](2), float64(2), int32(10), int64(4)\n",
      "memory usage: 117.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info() #Gives the dataframe information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd64181",
   "metadata": {},
   "source": [
    "# 5. Check for the Null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e0c87e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUSTOMER_ORDER_ID             0\n",
       "SALES_ORG                     0\n",
       "DISTRIBUTION_CHANNEL          0\n",
       "DIVISION                      0\n",
       "RELEASED_CREDIT_VALUE         0\n",
       "PURCHASE_ORDER_TYPE           0\n",
       "COMPANY_CODE                  0\n",
       "ORDER_CREATION_DATE           0\n",
       "ORDER_CREATION_TIME           0\n",
       "CREDIT_CONTROL_AREA           0\n",
       "SOLD_TO_PARTY                 0\n",
       "ORDER_AMOUNT                  0\n",
       "REQUESTED_DELIVERY_DATE       0\n",
       "ORDER_CURRENCY                0\n",
       "CREDIT_STATUS                 0\n",
       "CUSTOMER_NUMBER               0\n",
       "amount_in_usd              1377\n",
       "unique_cust_id                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #Checking and calculating the null values in each row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f806a",
   "metadata": {},
   "source": [
    "# 6. Replace all the null values with \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93c87503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with \"NaN\"\n",
    "df.fillna(\"NaN\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651708ed",
   "metadata": {},
   "source": [
    "# 7. Change the format of date columns - \"ORDER_CREATION_DATE\" to datetime[64] with the format as \"%Y%m%d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e83016d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATION_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATION_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], downcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Applying the date formatter\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATION_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mORDER_CREATION_DATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Printing the head of the df\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[71], line 6\u001b[0m, in \u001b[0;36mdate_format\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m      4\u001b[0m month \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m6\u001b[39m]) \u001b[38;5;66;03m#Extract the next two character stored in x that represents month\u001b[39;00m\n\u001b[0;32m      5\u001b[0m day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m6\u001b[39m:]) \u001b[38;5;66;03m#Extract the next two character stored in x that represents day\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "def date_format(date): #Represent the date \"YYYYMMDD\" which we want to convert\n",
    "    x = str(date) #Input the date and convert it in the string and store in variable x\n",
    "    year = int(x[0:4]) #Extract the four character stored in x that represents year\n",
    "    month = int(x[4:6]) #Extract the next two character stored in x that represents month\n",
    "    day = int(x[6:]) #Extract the next two character stored in x that represents day\n",
    "    return dt.datetime(year, month, day) #Return the date in YYYY-MM-DD format\n",
    "\n",
    "\n",
    "# Check if 'ORDER_CREATION_DATE' column exists in the DataFrame\n",
    "if 'ORDER_CREATION_DATE' in df.columns:\n",
    "    # Drop rows with missing values in 'ORDER_CREATION_DATE' column\n",
    "    df.dropna(subset=['ORDER_CREATION_DATE'], inplace=True)\n",
    "\n",
    "    # Down-casting date columns from float to int\n",
    "    df['ORDER_CREATION_DATE'] = pd.to_numeric(df['ORDER_CREATION_DATE'], downcast='integer')\n",
    "\n",
    "    # Applying the date formatter\n",
    "    df['ORDER_CREATION_DATE'] = df['ORDER_CREATION_DATE'].apply(date_format)\n",
    "\n",
    "    # Printing the head of the df\n",
    "    print(df.head(20))\n",
    "else:\n",
    "    print(\"The 'ORDER_CREATION_DATE' column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dd864",
   "metadata": {},
   "source": [
    "# 8. Do the same activity for the other date field i.e. \"REQUESTED_DELIVERY_DATE\" to datetime[64] with the format as \"%Y%m%d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38057719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_format(date): #Represent the date \"YYYYMMDD\" which we want to convert\n",
    "    x = str(date) #Input the date and convert it in the string and store in variable x\n",
    "    year = int(x[0:4]) #Extract the four character stored in x that represents year\n",
    "    month = int(x[4:6]) #Extract the next two character stored in x that represents month\n",
    "    day = int(x[6:])  #Extract the next two character stored in x that represents day\n",
    "    return dt.datetime(year, month, day) #Return the date in YYYY-MM-DD format\n",
    "\n",
    "\n",
    "# Down-casting date columns from float to int\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_numeric(df['REQUESTED_DELIVERY_DATE'], downcast='integer')\n",
    "\n",
    "# Applying the date formatter\n",
    "df['REQUESTED_DELIVERY_DATE'] = df['REQUESTED_DELIVERY_DATE'].apply(date_format)\n",
    "\n",
    "# Printing the head of the df\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ORDER_CREATION_DATE\", \"REQUESTED_DELIVERY_DATE\"]].dtypes #Returns the datatype of the specified column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68b9f5",
   "metadata": {},
   "source": [
    "# 9.Sanity check - Check how many records are having order date greater than the delivery date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc318e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the date columns to datetime format if needed\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Filter the records where order date is greater than delivery date\n",
    "filtered_records = df[df['ORDER_CREATION_DATE'] > df['REQUESTED_DELIVERY_DATE']]\n",
    "\n",
    "# Count the number of filtered records\n",
    "count = len(filtered_records)\n",
    "\n",
    "print(f\"Number of records where order date is greater than delivery date: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37ca50",
   "metadata": {},
   "source": [
    "# 10.Remove those records where order date is greater than the delivery date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime format if needed\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Filter the records where order date is less than or equal to delivery date\n",
    "filtered_dataset = df[df['ORDER_CREATION_DATE'] <= df['REQUESTED_DELIVERY_DATE']]\n",
    "\n",
    "# # Save the filtered dataset to a new CSV file\n",
    "# filtered_dataset.to_csv('filtered_dataset.csv', index=False)\n",
    "\n",
    "# Optional: Display the number of removed records\n",
    "removed_records = len(df) - len(filtered_dataset)\n",
    "print(f\"Number of removed records: {removed_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b48d7d",
   "metadata": {},
   "source": [
    "# 11.Check the number of records where the “ORDER_AMOUNT” field is having “-” in it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of records where ORDER_AMOUNT has \"-\"\n",
    "count = (df['ORDER_AMOUNT'].astype(str).str.contains('-')).sum() #Select the specified column and convert it into string and check whether string contains \"-\"\n",
    "print(\"Number of records where ORDER_AMOUNT has '-':\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f655e5d",
   "metadata": {},
   "source": [
    "# 12.Replace “-” with “” from the “ORDER_AMOUNT” field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"-\" with an empty string in the ORDER_AMOUNT field\n",
    "df['ORDER_AMOUNT'] = df['ORDER_AMOUNT'].str.replace('-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd2df0",
   "metadata": {},
   "source": [
    "# 13.Check the number of records where the “ORDER_AMOUNT” field is having “,” in it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of records where ORDER_AMOUNT has \",\"\n",
    "count = (df['ORDER_AMOUNT'].astype(str).str.contains(',')).sum()\n",
    "print(\"Number of records where ORDER_AMOUNT has ',':\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489c74f",
   "metadata": {},
   "source": [
    "# 14.Replace “,” with “.” from the “ORDER_AMOUNT” field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \",\" with an \".\" in the ORDER_AMOUNT field\n",
    "df['ORDER_AMOUNT'] = df['ORDER_AMOUNT'].str.replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80564a4e",
   "metadata": {},
   "source": [
    "# 15.Count the number of records where the order date and the delivery date are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a56c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of records where order date and delivery date are the same\n",
    "count = len(df[df['ORDER_CREATION_DATE'] == df['REQUESTED_DELIVERY_DATE']])\n",
    "\n",
    "print(f\"Number of records where order date and delivery date are the same: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128caa1",
   "metadata": {},
   "source": [
    "# 16.Count the number of records for each currency type by using the field “'ORDER_CURRENCY'”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c23e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ORDER_CURRENCY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f890f",
   "metadata": {},
   "source": [
    "# 17.Create a new column in the existing dataframe as “'amount_in_usd'” and convert all the non-USD currencies in USD and store them in the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff94984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #Importing request library for sending HTTP request to API\n",
    "\n",
    "# Define the currency conversion API endpoint\n",
    "conversion_api = 'https://api.exchangerate-api.com/v4/latest/USD'\n",
    "\n",
    "try:\n",
    "    # Make a GET request to the API endpoint\n",
    "    response = requests.get(conversion_api)\n",
    "    \n",
    "    if response.status_code == 200: #Indicate a successful request\n",
    "        # Extract the conversion rates from the API response\n",
    "        conversion_rates = response.json()['rates']\n",
    "        \n",
    "        # Convert 'ORDER_AMOUNT' column to numeric format\n",
    "        df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce') #errors='coerce' parameter handles any non-numeric values by converting them to NaN \n",
    "        \n",
    "        # Define a function to convert currency to USD\n",
    "        def convert_to_usd(row):\n",
    "            if row['ORDER_CURRENCY'] != 'USD':\n",
    "                if row['ORDER_CURRENCY'] in conversion_rates:\n",
    "                    return row['ORDER_AMOUNT'] / conversion_rates[row['ORDER_CURRENCY']]\n",
    "                else:\n",
    "                    return float('NaN')\n",
    "            else:\n",
    "                return row['ORDER_AMOUNT']\n",
    "        \n",
    "        # Create the 'amount_in_usd' column using the conversion function\n",
    "        df['amount_in_usd'] = df.apply(convert_to_usd, axis=1)\n",
    "        \n",
    "        # Display the updated DataFrame\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Error: Unable to retrieve currency conversion rates.\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error: \", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e4d00",
   "metadata": {},
   "source": [
    "# 18.Check for values “0” in the “'amount_in_usd” column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for values \"0\" in the \"amount_in_usd\" column\n",
    "zero_values = df[df['amount_in_usd'] == 0]\n",
    "\n",
    "if len(zero_values) > 0:\n",
    "    print(\"There are records with a value of 0 in the 'amount_in_usd' column.\")\n",
    "    print(zero_values)\n",
    "else:\n",
    "    print(\"No records found with a value of 0 in the 'amount_in_usd' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee6a10",
   "metadata": {},
   "source": [
    "# 19.Create a new column in the existing dataframe “unique_cust_id” by adding 'CUSTOMER_NUMBER' and 'COMPANY_CODE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c44127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'CUSTOMER_NUMBER' to string type\n",
    "df['CUSTOMER_NUMBER'] = df['CUSTOMER_NUMBER'].astype(str)\n",
    "\n",
    "# Convert 'COMPANY_CODE' to string type (if necessary)\n",
    "df['COMPANY_CODE'] = df['COMPANY_CODE'].astype(str)\n",
    "\n",
    "# Create the 'unique_cust_id' column by combining 'CUSTOMER_NUMBER' and 'COMPANY_CODE'\n",
    "df['unique_cust_id'] = df['CUSTOMER_NUMBER'] + df['COMPANY_CODE']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc85f5",
   "metadata": {},
   "source": [
    "# Milestone 2 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d9da",
   "metadata": {},
   "source": [
    "# 1.Create a Histogram on DISTRIBUTION_CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8efbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #matlab library is used for plotting different graphs \n",
    "\n",
    "# Create a histogram of the 'DISTRIBUTION_CHANNEL' column\n",
    "plt.figure(figsize=(35, 7))  # Set the figure size\n",
    "df['DISTRIBUTION_CHANNEL'].value_counts().plot(kind='bar')  # Plot the histogram\n",
    "plt.xlabel('Distribution Channel', fontsize = 12)  # Set the x-label\n",
    "plt.ylabel('Count', fontsize = 12)  # Set the y-label\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.title('Distribution of Distribution Channels')  # Set the title\n",
    "plt.show()  # Show the histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f7e1c",
   "metadata": {},
   "source": [
    "# 2.Create a Pie Chart on ORDER_CURRENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart of the 'ORDER_CURRENCY' column\n",
    "plt.figure(figsize=(8, 8))  # Set the figure size\n",
    "df['ORDER_CURRENCY'].value_counts().plot(kind='pie', autopct='%1.1f%%')  # Plot the pie chart\n",
    "plt.title('Distribution of Order Currencies')  # Set the title\n",
    "plt.yticks(rotation=45)\n",
    "plt.ylabel('', fontsize = 10)  # Remove the y-label\n",
    "plt.show()  # Show the pie chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d1f7e",
   "metadata": {},
   "source": [
    "# 3.Create a line chart PURCHASE_ORDER_TYPE and DISTRIBUTION_CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'PURCHASE_ORDER_TYPE' and 'DISTRIBUTION_CHANNEL' and count the occurrences\n",
    "grouped_data = df.groupby(['PURCHASE_ORDER_TYPE', 'DISTRIBUTION_CHANNEL']).size().unstack() #unstack() function is then used to pivot the grouped data, converting the hierarchical index into columns\n",
    "\n",
    "# Plot a line chart for each 'PURCHASE_ORDER_TYPE'\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "for purchase_order_type in grouped_data.columns:\n",
    "    plt.plot(grouped_data.index, grouped_data[purchase_order_type], marker='o', label=purchase_order_type)\n",
    "\n",
    "plt.xlabel('Distribution Channel')  # Set the x-label\n",
    "plt.ylabel('Count')  # Set the y-label\n",
    "plt.title('Distribution of Purchase Order Types by Distribution Channel')  # Set the title\n",
    "plt.legend()  # Show the legend\n",
    "plt.show()  # Show the line chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcc986",
   "metadata": {},
   "source": [
    "# 4.Create a line plot on ORDER_CREATION_DATE and amount_in_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ORDER_CREATION_DATE' column to datetime type\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "\n",
    "# Sort the DataFrame by 'ORDER_CREATION_DATE'\n",
    "df.sort_values('ORDER_CREATION_DATE', inplace=True)\n",
    "\n",
    "# Plot the line chart\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size\n",
    "plt.plot(df['ORDER_CREATION_DATE'], df['amount_in_usd'], marker='o', linestyle='-', color='blue')\n",
    "\n",
    "plt.xlabel('Order Creation Date')  # Set the x-label\n",
    "plt.ylabel('Amount in USD')  # Set the y-label\n",
    "plt.title('Order Amount in USD Over Time')  # Set the title\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.grid(True)  # Show gridlines\n",
    "plt.show()  # Show the line chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a003bdd",
   "metadata": {},
   "source": [
    "# 5. Create a boxplot on ORDER_AMOUNT to find out the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #Seaborn is a data visualization library.Improved Styling, High-Level Plotting Functions,Statistical Estimation\n",
    "# Plot the boxplot\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "sns.boxplot(x=df['ORDER_AMOUNT'], color ='blue')\n",
    "plt.xlabel('Order Amount')  # Set the x-label\n",
    "plt.title('Boxplot of Order Amount')  # Set the title\n",
    "plt.show()  # Show the boxplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b85c3",
   "metadata": {},
   "source": [
    "# 6.Create a barchart on COMPANY_CODE and ORDER_AMOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'COMPANY_CODE' and calculate the sum of 'ORDER_AMOUNT'\n",
    "grouped_data = df.groupby('COMPANY_CODE')['ORDER_AMOUNT'].sum()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "grouped_data.plot(kind='bar', color='blue')\n",
    "\n",
    "plt.xlabel('Company Code')  # Set the x-label\n",
    "plt.ylabel('Total Order Amount')  # Set the y-label\n",
    "plt.title('Total Order Amount by Company Code')  # Set the title\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.show()  # Show the bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f38608",
   "metadata": {},
   "source": [
    "# Milestone 3 - Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02811ba6",
   "metadata": {},
   "source": [
    "# 1.Check for the outliers in the “amount_in_usd” column and replace the outliers with appropriate values, discussed in the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc678da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate the z-score for the \"amount_in_usd\" column\n",
    "z_scores = np.abs(stats.zscore(df['amount_in_usd']))\n",
    "\n",
    "# Set a threshold for outlier detection (e.g., z-score greater than 3)\n",
    "outlier_threshold = 3\n",
    "\n",
    "# Detect outliers using the z-score\n",
    "outliers = df[z_scores > outlier_threshold]\n",
    "\n",
    "# Calculate the IQR for the \"amount_in_usd\" column\n",
    "Q1 = df['amount_in_usd'].quantile(0.25)\n",
    "Q3 = df['amount_in_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds for outlier detection using the IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Detect outliers using the IQR method\n",
    "outliers = df[(df['amount_in_usd'] < lower_bound) | (df['amount_in_usd'] > upper_bound)]\n",
    "\n",
    "# Replace outliers with appropriate values (e.g., median of the column)\n",
    "median_value = df['amount_in_usd'].median()\n",
    "df.loc[z_scores > outlier_threshold, 'amount_in_usd'] = median_value\n",
    "df.loc[(df['amount_in_usd'] < lower_bound) | (df['amount_in_usd'] > upper_bound), 'amount_in_usd'] = median_value\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea46b9",
   "metadata": {},
   "source": [
    "# 2.Label encoding or One hot Encoding on all the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_categorical_columns(df):\n",
    "    # Create an empty list to store the categorical columns\n",
    "    categorical_columns = []\n",
    "    \n",
    "     # Iterate over each column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Check if the column's dtype is 'object' (string) or if it is a categorical dtype\n",
    "        if df[column].dtype == 'object' or pd.api.types.is_categorical_dtype(df[column]):\n",
    "            # If the column is categorical, add it to the list\n",
    "            categorical_columns.append(column)\n",
    "    # Return the list of categorical columns        \n",
    "    return categorical_columns\n",
    "\n",
    "# Assuming 'df' is dataframe\n",
    "categorical_columns = identify_categorical_columns(df)\n",
    "print(\"Categorical columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# Define the categorical columns to be encoded\n",
    "categorical_columns = ['DISTRIBUTION_CHANNEL', 'DIVISION',\n",
    "                       'RELEASED_CREDIT_VALUE', 'PURCHASE_ORDER_TYPE', 'COMPANY_CODE',\n",
    "                       'CREDIT_CONTROL_AREA', 'ORDER_CURRENCY',\n",
    "                       'CREDIT_STATUS', 'CUSTOMER_NUMBER', 'unique_cust_id']\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Perform label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    # Convert float values to strings\n",
    "    df_encoded[column] = df_encoded[column].astype(str)\n",
    "    df_encoded[column] = label_encoder.fit_transform(df_encoded[column])\n",
    "\n",
    "# Display the encoded dataframe\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bf441",
   "metadata": {},
   "source": [
    "# 3.Log Transformations on continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_continuous_columns(df, threshold=0.1):\n",
    "    continuous_columns = []\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]): # Check if the column has a numeric data type\n",
    "            unique_values_ratio = len(df[column].unique()) / len(df[column]) # Calculate the ratio of unique values to total values\n",
    "            if unique_values_ratio > threshold: # Check if the ratio exceeds the threshold\n",
    "                continuous_columns.append(column) # Add the column to the list of continuous columns\n",
    "    return continuous_columns\n",
    "\n",
    "# Assuming 'df' is dataset\n",
    "continuous_columns = identify_continuous_columns(df)\n",
    "print(\"Continuous columns:\", continuous_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5395d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the continuous columns to be transformed\n",
    "continuous_columns = ['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT', 'amount_in_usd']\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Perform log transformations\n",
    "for column in continuous_columns:\n",
    "    if np.issubdtype(df_transformed[column].dtype, np.number):\n",
    "        # Handle zero or negative values\n",
    "        mask = (df_transformed[column] <= 0)\n",
    "        df_transformed.loc[~mask, column] = np.log(df_transformed.loc[~mask, column])\n",
    "\n",
    "# Display the transformed dataframe\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0a5b6",
   "metadata": {},
   "source": [
    "# 4.Try to extract new features by grouping existing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_existing_columns(df):\n",
    "    existing_columns = df.columns.tolist()\n",
    "    return existing_columns\n",
    "\n",
    "# Assuming 'df' is dataset\n",
    "existing_columns = identify_existing_columns(df)\n",
    "print(\"Existing columns:\", existing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7346f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping existing columns and creating new features\n",
    "df_grouped = df.groupby(['CUSTOMER_ORDER_ID', 'CUSTOMER_NUMBER']).agg({\n",
    "    'SALES_ORG': 'mean',\n",
    "    'ORDER_AMOUNT': 'sum',\n",
    "    'amount_in_usd': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the new columns\n",
    "df_grouped = df_grouped.rename(columns={\n",
    "    'SALES_ORG': 'NEW_MEAN',\n",
    "    'ORDER_AMOUNT': 'NEW_SUM',\n",
    "    'amount_in_usd': 'NEW_COUNT'\n",
    "})\n",
    "\n",
    "# Display the grouped dataframe with new features\n",
    "print(df_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f38ce",
   "metadata": {},
   "source": [
    "# 5.Create a heatmap to find correlation between the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Create the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cc02f",
   "metadata": {},
   "source": [
    "# 6.Try to identify important or relevant columns for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19565e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "def identify_correlated_columns(df, threshold=0.5):\n",
    "    numeric_columns = df.select_dtypes(include='number').columns\n",
    "    corr_matrix = df[numeric_columns].corr().abs()\n",
    "    upper_tri = corr_matrix.where(~np.tril(np.ones(corr_matrix.shape), k=-1).astype(bool))\n",
    "    correlated_columns = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    return correlated_columns\n",
    "\n",
    "# Assuming 'df' is dataset\n",
    "correlated_columns = identify_correlated_columns(df)\n",
    "print(\"Correlated columns:\", correlated_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11892e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert float values in categorical columns to strings\n",
    "df[categorical_columns] = df[categorical_columns].astype(str)\n",
    "\n",
    "# Encode categorical columns with label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Remove datetime columns from the feature set\n",
    "datetime_columns = df.select_dtypes(include=['datetime64']).columns\n",
    "X = df.drop(['REQUESTED_DELIVERY_DATE'] + list(datetime_columns), axis=1)  # Replace with the actual target column name\n",
    "\n",
    "# Extract the target variable\n",
    "y = df['ORDER_AMOUNT']\n",
    "\n",
    "# Create an imputer to fill missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Convert X back to a DataFrame\n",
    "X = pd.DataFrame(X, columns=df.drop(['REQUESTED_DELIVERY_DATE'] + list(datetime_columns), axis=1).columns)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Fit the model to the data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top 20 most important features\n",
    "print(importance_df.head(20))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2134",
   "metadata": {},
   "source": [
    "# Note - All of you Introduce a particular module (Link Attached) for datetime manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_in_days(melt, lags, ffday, CUSTOMER_ORDER_ID, ORDER_CREATION_DATE, ORDER_AMOUNT):\n",
    "    for i in range(ffday, lags+1):\n",
    "        melt['Last-'+str(i)+'day_Sales'] = melt.groupby([CUSTOMER_ORDER_ID])[ORDER_AMOUNT].shift(i)\n",
    "    \n",
    "    melt = melt.reset_index(drop = True)\n",
    "    \n",
    "    for i in range(ffday, lags+1):\n",
    "        melt['Last-'+str(i)+'day_Diff']  = melt.groupby([CUSTOMER_ORDER_ID])['Last-'+str(i)+'day_Sales'].diff()\n",
    "    melt = melt.fillna(0)\n",
    "    return melt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb9d08",
   "metadata": {},
   "source": [
    "# Milestone 4 - ML Models and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bac6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting back to antilog transformation\n",
    "\n",
    "# Define the continuous columns to be transformed\n",
    "continuous_columns = ['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT', 'amount_in_usd']\n",
    "\n",
    "# Create a copy of the transformed dataframe\n",
    "df_antilog = df_transformed.copy()\n",
    "\n",
    "# Perform antilog transformations\n",
    "for column in continuous_columns:\n",
    "    if np.issubdtype(df_antilog[column].dtype, np.number):\n",
    "        # Handle zero or negative values\n",
    "        mask = (df_antilog[column] <= 0)\n",
    "        df_antilog.loc[~mask, column] = np.exp(df_antilog.loc[~mask, column])\n",
    "\n",
    "# Display the antilog transformed dataframe\n",
    "print(df_antilog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e6f92",
   "metadata": {},
   "source": [
    "# 1. Modify the dataset to pass into any type of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb469b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df['amount_in_usd']\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.fillna(0)  # Replace 0 with an appropriate value or strategy for handling missing values\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    X[col] = encoder.fit_transform(X[col])\n",
    "\n",
    "# Scale numerical features (if required)\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust the test_size as desired\n",
    "\n",
    "# Now you can use X_train, X_test, y_train, y_test with your machine learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21490e3",
   "metadata": {},
   "source": [
    "# 2.Try different machine learning models like -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa96df",
   "metadata": {},
   "source": [
    "# a. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca27a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the imputer for the features\n",
    "imputer = SimpleImputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Access the model's coefficients and intercept\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d614507",
   "metadata": {},
   "source": [
    "# b. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Reshape the target variable\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "# Create and train the Linear SVR model\n",
    "model = LinearSVR()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a26b69",
   "metadata": {},
   "source": [
    "# c. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daab912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977805d4",
   "metadata": {},
   "source": [
    "# d. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Reshape y_train to a 1-dimensional array\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bc8f2",
   "metadata": {},
   "source": [
    "# e. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ca9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Create and train the AdaBoost model\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46823dd",
   "metadata": {},
   "source": [
    "# f. Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1df0f3",
   "metadata": {},
   "source": [
    "# 3. Perform Regression model evaluations like MSE, RMSE, R-Square etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Define the columns to be imputed\n",
    "impute_cols_X = X.columns\n",
    "impute_cols_y = y.name\n",
    "\n",
    "# Create a ColumnTransformer to apply imputation only on specific columns\n",
    "imputer_X = SimpleImputer(strategy='mean')\n",
    "imputer_y = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation on the features\n",
    "X_imputed = imputer_X.fit_transform(X)\n",
    "\n",
    "# Apply imputation on the target variable\n",
    "y_imputed = imputer_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-Squared:\", r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the input features (all columns except the target variable)\n",
    "input_features = df.columns.tolist()\n",
    "target_variable = 'amount_in_usd'  # Replace with the actual name of your target variable column\n",
    "input_features.remove(target_variable)\n",
    "\n",
    "# Print the input features\n",
    "print(\"Input Features:\", input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Handle missing values in the features\n",
    "imputer = SimpleImputer()\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Ridge Regression model\n",
    "model = Ridge(alpha=0.5)  # Set the alpha value (regularization strength)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared (R^2):\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in X_train and X_test\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Create and train the Lasso Regression model\n",
    "model = Lasso(alpha=0.5)  # Set the alpha value (regularization strength)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for features and target variable\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Elastic Net Regression model\n",
    "model = ElasticNet(alpha=0.5, l1_ratio=0.5)  # Set the alpha and l1_ratio values\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Create and train the Decision Tree Regression model\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared (R^2):\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest Regression model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared (R^2):\", r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b72a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Impute missing values in X_train\n",
    "imputer = SimpleImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "# Create and train the LinearSVR model with adjusted parameters\n",
    "model = LinearSVR(max_iter=5000, tol=1e-4)  # Increase the number of iterations\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "X_test = imputer.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-Squared (R^2):\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45af847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Impute missing values in X\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the imputed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Adaboost model\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-Squared (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Impute missing values in X\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the imputed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Xgboost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-Squared (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc317d",
   "metadata": {},
   "source": [
    "# 4. Compare the accuracies of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0967e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform imputation separately on training and testing sets for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_train = imputer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test = imputer.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Impute missing values in the feature matrix\n",
    "imputer = SimpleImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    LinearSVR(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    xgb.XGBRegressor()\n",
    "]\n",
    "\n",
    "# Evaluate each model and compare their performance\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {type(model).__name__}\")\n",
    "    print(f\"MSE: {mse:}\")\n",
    "    print(f\"RMSE: {rmse:}\")\n",
    "    print(f\"MAE: {mae:}\")\n",
    "    print(f\"R-Squared: {r2:}\")\n",
    "    print(\"----------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55d470",
   "metadata": {},
   "source": [
    "# 5. Select the best possible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48307b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for both feature matrix and target variable\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    LinearSVR(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    xgb.XGBRegressor()\n",
    "]\n",
    "\n",
    "# Perform cross-validation and evaluate each model\n",
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    average_score = scores.mean()\n",
    "    \n",
    "    if average_score > best_score:\n",
    "        best_model = model\n",
    "        best_score = average_score\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model: {type(best_model).__name__}\")\n",
    "print(f\"MSE: {mse:}\")\n",
    "print(f\"RMSE: {rmse:}\")\n",
    "print(f\"MAE: {mae:}\")\n",
    "print(f\"R-Squared: {r2:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb71e7e",
   "metadata": {},
   "source": [
    "# 6.Perform Hyperparameter tuning, select best hyperparameters by using appropriate algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning is performed to optimize the performance of a machine learning model by finding the best combination \n",
    "# of hyperparameters.\n",
    "# -Improving model performance\n",
    "# -Avoiding overfitting\n",
    "# -Model customization\n",
    "# -Enhancing model interpretability\n",
    "# -Finding the best trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "# Import the necessary libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Filter and ignore the ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using a simple imputer\n",
    "imputer = SimpleImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [1.0, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the decision tree regressor with the initial hyperparameters\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Perform grid search cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the testing set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance using mean squared error (MSE), mean absolute error (MAE), and R-squared (R^2)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the best hyperparameters and the evaluation metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-Squared (R^2):\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost Regression\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "df['ORDER_CREATION_DATE'] = pd.to_datetime(df['ORDER_CREATION_DATE'])\n",
    "df['REQUESTED_DELIVERY_DATE'] = pd.to_datetime(df['REQUESTED_DELIVERY_DATE'])\n",
    "\n",
    "# Convert target variables to numeric format\n",
    "df['amount_in_usd'] = pd.to_numeric(df['amount_in_usd'], errors='coerce')\n",
    "df['ORDER_AMOUNT'] = pd.to_numeric(df['ORDER_AMOUNT'], errors='coerce')\n",
    "\n",
    "# Drop datetime columns from the dataframe\n",
    "df_numeric = df.drop(['ORDER_CREATION_DATE', 'REQUESTED_DELIVERY_DATE'], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df_numeric.drop(['CUSTOMER_ORDER_ID', 'SOLD_TO_PARTY', 'ORDER_AMOUNT'], axis=1)\n",
    "y = df_numeric['amount_in_usd']\n",
    "\n",
    "# Perform imputation for target variable\n",
    "imputer = SimpleImputer()\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the model with best hyperparameters\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30880be",
   "metadata": {},
   "source": [
    "# 7.Come up with the best possible model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0f365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to me:\n",
    "    \n",
    "#     Best possible model is : Xgboost Regression\n",
    "        \n",
    "#         Reason : XGBoost is known for its exceptional performance and is often considered a state-of-the-art algorithm for \n",
    "#             gradient boosting. For large dataset we need high predictive accuracy, and are less concerned about \n",
    "#             interpretability.So I prefered Xgboost Regression over Decision tree Regression because Decision tree is more \n",
    "#             concerned for interpretability.\n",
    "            \n",
    "#             Accuracy before Hypertuning : 0.999998\n",
    "#                 Accuracy After Hypertuning : 0.99999998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bda196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
